---
title: 自瞄系统演进记录（Aim Assistant Journey）
version: v1.1 - v1.1_re
status: in-progress
category: journey
last_updated: 2025-11-23
history_scope: 记录自 v1.1 起自瞄系统从简化方案到当前 selector/angle/pipeline 架构的演进过程。
history_period: 2024-01-01 ~ 2025-11-23
history_tags:
  - aimassistant
  - autoaim
  - vision
  - gimbal
  - performance
llm_summary: >-
  本文按时间与决策脉络，记录自瞄系统从最初
  “先能打”的简化方案演进到当前 selector/angle/pipeline
  架构的过程，以及与 autoaim 技能、视觉性能、云台控制
  和全局配置之间的互动。重点包括：为何选择简化方案、
  selector/angle/pipeline 的拆分缘由，自瞄技能中的搜索
  策略与步进角设计，在有限推理 FPS 约束下的权衡，以及
  多个已知坑点和未来改造方向。
llm_entry_points:
  - 需要理解当前 selector/angle/pipeline 架构的历史来源与取舍时。
  - 排查自瞄偶尔丢目标、乱转圈或瞄不准等问题时。
  - 计划在更高 FPS 或不同摄像头下重写/调整自瞄策略时。
related_docs:
  - docs/general_intro.md
  - docs/index.md
  - docs/status.md
  - docs/plans.md
  - docs/journey/v1_0_history.md
  - docs/journey/vision_recognition_journey.md
  - docs/journey/uart_communication_journey.md
entry_points:
  - 自瞄
  - aimassistant
  - autoaim skill
  - selector
  - angle
  - pipeline
---

# 自瞄系统演进记录（Aim Assistant Journey）

> 本文是 **自瞄系统（Aim Assistant）** 的模块级演进记录，
> 重点讲“为什么今天的代码长成现在这样”，而不是“当前接口怎么用”。
> 
> - 如果你是第一次接触整个项目，请先阅读 `docs/general_intro.md`。
> - 如果你只想知道“现在自瞄能做什么”，请优先查看 `docs/status.md` 与 `docs/plans.md`。
> - 本文更适合作为：重构、排错和参数大改之前的“历史考古”参考。

---

## 1. 自瞄需求与整体路线回顾

本节回顾自瞄从“没有闭环”到“必须打通一条可用链路”的背景，以及在 v1.1 阶段做出的路线选择。

### 1.1 从 v1.0 的“只有框架”到需要闭环自瞄

在 v1.0 阶段（详见 `docs/journey/v1_0_history.md`），项目主要完成了：

- 串口通信与硬件抽象层（`src/bot/`）；
- 技能系统框架（`src/skill/`）与 REPL 工具（`src/repl.py`）；
- 基础视觉识别模块雏形（早期版 `src/recognizer.py`）。

当时**并没有真正意义上的自瞄闭环**：

- 视觉只能给出检测框等信息；
- 技能可以根据键盘事件调用底盘/云台接口；
- 但“如何根据检测结果控制云台并在合适时机开火”的完整链路尚未落地。

v1.1 之后，随着比赛/实验需求明确，自瞄被提上了日程：

- 场景要求：在 RoboMaster EP/S1 上实现“自动对准目标并射击”的基础能力；
- 第一阶段目标：**先能打**，不追求复杂弹道补偿和多目标策略；
- 约束前提：
  - 视觉推理 FPS 有明显上限；
  - 云台控制接口来自底层 SDK，需要遵守角度/速度约束；
  - 项目整体还要兼顾稳定性与可维护性。

### 1.2 复杂方案 VS 简化方案的早期博弈

在旧设计草案中，自瞄曾考虑过更复杂的路线：

- 引入单目测距模块，根据装甲板实际尺寸和相机内参估算距离；
- 使用卡尔曼滤波等方法进行目标轨迹预测；
- 在云台侧增加 PID 控制器，根据误差闭环调整角速度；
- 甚至规划了 `AngleSolver` 类和更复杂的 pipeline 结构。

但结合时间、人力和现有代码基础，最终在 v1.1 阶段确定了一条**简化路线**：

- 不实现测距、卡尔曼和 PID，只依赖 SDK 自带控制逻辑；
- 自瞄只负责：
  - 从一批检测框中选择当前目标；
  - 根据相机 FOV 和框中心估算需要调整的 yaw/pitch；
  - 通过 `bot/gimbal` 发送相对角度控制命令；
- 其余细节（如弹道、微小抖动的补偿）暂时交给人或未来版本。

这条简化路线直接催生了后面要讲的 `selector.py`、`angle.py` 和 `pipeline.py` 三件套。

---

## 2. 从简化方案到 selector / angle / pipeline 三分模块

本节记录自瞄核心逻辑是如何从“一个大循环”逐步拆分为 `selector`、`angle` 和 `pipeline` 三个文件的，以及中间做过的取舍。

> 相关源码：
> - `src/aimassistant/selector.py`
> - `src/aimassistant/angle.py`
> - `src/aimassistant/pipeline.py`

### 2.1 selector_v1：用评分因子从检测框中选目标

`selector.py` 的目标很直接：

- 输入：来自 `recognizer` 的一批检测框（通常包含类别、置信度、归一化 xywh 等信息）；
- 输出：当前这一帧里“最值得瞄”的那个框。

为此，当前代码采用了 **因子 + 权重** 的方式：

- 使用 `@register_selector_factor` 装饰器注册多个“评分因子”；
- 强制所有因子权重的和不超过 1.0，以避免后续随意叠加权重导致评分失真；
- 当前 v1 版本主要三个因子：
  - 面积因子：用归一化 w*h 代表目标大小，粗略体现远近；
  - 中心因子：根据检测框中心与图像中心的距离计算“居中程度”；
  - 置信度因子：利用模型输出置信度，避免瞄准明显错误的框。

这里有两个需要特别记录的历史坑点：

- **中心距离的定义问题**：
  - 直观上，我们希望用“距离图像中心 (0.5, 0.5) 有多远”来衡量目标偏离视野中心的程度；
  - 但在早期实现中，计算时采用了“到 (0, 0) 的距离”，这在数学上并不符合“居中”的直觉；
  - 这一实现目前仍然存在，是未来重构 selector 时需要注意的遗留问题。
- **权重约束的扩展性问题**：
  - 当前通过全局装饰器累加权重的设计，对 v1 来说是安全的；
  - 但未来如果继续添加新的评分因子，很容易在不知情的情况下超过 1.0 而触发异常；
  - journey 文档在此处做记录，是为了给之后可能的“基于配置文件定义权重”的改造埋下伏笔。

### 2.2 angle 模块：从检测框到 yaw/pitch 偏差

`angle.py` 负责把 selector 选出的目标框转成“云台应该转多少”的信息。

核心思路是：

- 使用 **相机水平视场角 (CAMERA_FOV_HORIZONTAL)** 与图像宽高；
- 将 YOLO 输出的归一化中心坐标 (x, y) 映射到相对于图像中心的偏移；
- 再根据水平/垂直 FOV 计算 yaw/pitch 角度偏差。

实现中的关键点：

- 为了得到垂直 FOV，代码使用了一个常见公式：
  - 从水平 FOV 和图像宽高比推导出垂直 FOV；
  - 这依赖于“像素缩放比例一致”的假设，现实中并不总是严格成立。
- pitch 方向的符号：
  - 图像坐标 y 轴向下增加，但抬枪是正 pitch；
  - 代码中采用了相应的符号约定来修正这一差异。

这里也有一个需要明确记下的事实：

- 当前 `CAMERA_FOV_HORIZONTAL`、`CAMERA_WIDTH`、`CAMERA_HEIGHT` 等参数，
  多数是结合常见相机参数和少量经验设定的“典型值”，
  并没有经过严格的标定流程；
- 自瞄精度的上限在很大程度上由这些参数决定，
  这也是未来需要单独补充“标定工具/流程”和相关 journey 的原因之一。

### 2.3 pipeline：把 selector 与 angle 串起来

`pipeline.py` 提供了 `aim_step_v1(...)` 这样的函数，用来完成单次自瞄逻辑：

1. 从 `recognizer` 的结果中取出检测框列表；
2. 如果当前没有任何有效目标，直接返回一个“无目标”的信号；
3. 调用 selector 选出当前帧最优目标框；
4. 调用 angle 模块计算出 yaw/pitch 偏移；
5. 通过 `bot/gimbal` 接口发送一次相对角度控制命令。

值得一提的是，早期设计中这里曾经是一个 `AimAssistantPipeline` 类：

- 类内持有状态，例如连续丢失目标帧数、上一次目标信息等；
- 逻辑较为集中，但也因此更难在技能层面灵活组合。

后来在 v1.1_re 重构中，这部分被简化为 **无状态函数**：

- 状态（例如丢失帧计数）改由 `autoaim_action` 等上层逻辑管理；
- pipeline 本身只专注于“当前这一帧”的计算逻辑；
- 这样做的好处是职责更清晰，自瞄行为的节奏可以更自由地在技能层控制。

另外，出于安全与阶段目标考虑：

- pipeline 目前**只负责瞄准，不直接触发发射器**；
- 何时开火、如何控制发射节奏，可以在技能或更高一层的策略中单独设计。

---

## 3. 自瞄技能与搜索策略演进（`src/skill/autoaim.py`）

本节关注的是：自瞄是如何被包装成一个技能的，以及“找不到目标时怎么办”。

> 相关源码：
> - `src/skill/autoaim.py`
> - `src/skill/base_skill.py`
> - `src/skill/manager.py`

### 3.1 作为技能的自瞄：键位模型与生命周期

在技能系统（详见 `src/skill/base_skill.py` 与 `manager.py`）稳定之后，
自瞄被设计为一个可以通过按键开关的技能：

- 典型模式：按一次键开启自瞄，再按一次关闭；
- 技能内部通过一个线程循环（例如 `autoaim_action`）持续执行自瞄逻辑；
- 循环条件通常类似于 `while self.enabled`，由 `SkillManager` 统一管理 `enabled` 状态。

在早期实现中，曾经尝试在技能内部直接标记“错误状态”：

- 例如设想中的 `set_errored` 接口，用于在异常时通知管理器；
- 实际代码与接口设计并不完全匹配，容易造成状态不一致。

经历几轮迭代后，当前的共识是：

- 在技能线程内使用 try/except 包裹主循环；
- 一旦发生异常：记录日志，然后让线程自然退出；
- 具体的状态管理仍由 `SkillManager` 负责，避免出现“双重管理”的情况。

这一点在后续重构其它技能时也值得参考。

### 3.2 丢失目标：从“回中”到“旋转搜索”

自瞄在实际运行中很难保证每一帧都有目标：

- 目标可能暂时跑出画面；
- 识别模型可能在个别帧上失败；
- 赛场环境中可能存在遮挡。

早期方案是在“连续若干帧没有检测到目标”时：

- 直接让云台回中位置（例如 yaw=0, pitch=0 一类的姿态）；
- 这样实现简单，但在近战场景下体验并不好：
  - 云台频繁从一侧跳回中间，容易错过目标；
  - 回中过程本身也浪费时间。

在当前版本中，与其“频繁回中”，不如**主动搜索**：

- 在 `autoaim_action` 中维护一个 `lost_frames` 计数；
- 当 `lost_frames` 小于某个阈值（见 `config.AIM_LOST_TARGET_TIMEOUT_FRAMES`）时，
  仍然以“等待新识别结果”为主；
- 当连续丢失帧数超过阈值时，
  进入“搜索模式”：使云台在 yaw 方向以一定步进角度缓慢旋转。

阈值的具体数值在历史上发生过变化：

- v1.0/v1.1 早期草案中，曾用过较大的数字（例如 12 帧）；
- 实践中发现响应过慢，且容易在错误状态下停留太久；
- 之后调低到更敏捷的设置（如 5 帧），
  在“识别短抖动”和“真正目标消失”之间做出平衡。

### 3.3 搜索步进角：search_step_angle 的由来

在搜索模式中，自瞄需要决定每次旋转多少角度：

- 步进角太小：扫描一圈时间过长；
- 步进角太大：容易直接跨过目标，造成漏扫。

当前实现中，search_step_angle 一般会综合以下因素：

- 水平视场角 `CAMERA_FOV_HORIZONTAL`；
- 一个“覆盖率系数”，控制相邻视场多少重叠；
- 云台旋转速度 `GIMBAL_SPEED`；
- 实际推理 FPS（来自 `recognizer` 的状态信息，如 `actual_inference_fps`）。

历史上主要有两类约束思路：

1. **视场覆盖约束**：
   - 保证相邻搜索步之间的视场有足够重叠；
   - 例如单步不超过某个比例的 FOV，避免出现“黑带”漏扫。
2. **云台速度约束**：
   - 保证在一帧推理时间内云台物理上确实能转到指定角度；
   - 由 `GIMBAL_SPEED / FPS` 给出一个上限。

最终的 search_step_angle 往往是“两个约束中的较小值”：

- FPS 越低，云台能安全跨越的角度越小，自然会趋向更保守的步进；
- FPS 越高，可以适当增大步进角，提高扫描效率。

在当前视觉性能基线（大约 4 FPS 左右）下，
实测可用的 search_step_angle 一般在十几到二十度的范围，
转一圈大约需要不到 1 秒的时间，这在多数场景下是可接受的。

### 3.4 pitch 策略与云台接口使用

在搜索模式中，当前实现通常只在 yaw 方向做旋转：

- 搜索阶段不主动调整 pitch，
  避免云台上下乱晃影响操作者感知；
- 实际瞄准时，再在 angle 计算阶段综合 pitch 偏差，
  并通过 gimbal 控制接口一并下发。

从接口选择角度看：

- 回中等操作依赖 `bot/gimbal` 中的复合控制函数；
- 搜索阶段更偏向使用“按角速度或相对角度连续旋转”的接口；
- 这些细节会在未来的 `gimbal` 相关 journey 中进一步展开，
  本文只做高层记录。

---

## 4. 与视觉 / 性能 / 配置的互动

自瞄并不是孤立存在的，它深度依赖视觉推理性能、
硬件能力以及全局配置中的若干参数。本节以较高层次概括这些互动关系。

> 相关源码与文档：
> - `src/recognizer.py`
> - `src/config.py`
> - 旧文档：`old_docs/documents/history/PERFORMANCE_OPTIMIZATION_HISTORY.md` 等

### 4.1 视觉线程模型与实际 FPS

`src/recognizer.py` 中，一般采用“双线程模型”：

- 摄像头采集线程负责不断抓取最新帧；
- 推理线程在获取到最新帧后执行 ONNX 推理；
- 推理线程只处理最新帧，丢弃积压帧，以避免“排队导致巨大延迟”。

在早期版本中，存在一些典型性能坑：

- 在推理循环内部错误地使用 `time.sleep()`；
- 树莓派电源/散热不足导致 CPU 自动降频；
- 使用 NCNN 或不合适的推理后端导致性能不足等。

通过一系列调整，当前视觉链路在目标硬件上
基本可以稳定到 **约 4 FPS 量级**，
这也是 search_step_angle 等策略设计时的关键前提。

更详细的性能演进和调优故事，
会在未来的 `vision_recognition_journey.md` 中系统记录。

### 4.2 自瞄相关配置项的沉淀（`config.py`）

为了避免参数散落，自瞄相关的重要配置被收敛到 `src/config.py` 中，例如：

- `CAMERA_FOV_HORIZONTAL`：相机水平视场角；
- `CAMERA_WIDTH` / `CAMERA_HEIGHT`：图像分辨率，用于推导垂直 FOV；
- `GIMBAL_SPEED`：预期云台旋转速度（度/秒）；
- `AIM_LOST_TARGET_TIMEOUT_FRAMES`：丢失目标多少帧后进入搜索模式；
- 其他自瞄控制相关的常量。

当前这些配置：

- 多数基于经验和少量实测；
- 尚未经过大规模系统性标定；
- 在进行新硬件、不同镜头或极端场景测试时，
  需要结合本 journey 文档进行重新审视和调整。

### 4.3 性能约束对设计的“反推”

视觉 FPS、云台速度和串口带宽，
共同决定了自瞄策略可行的设计空间。

在历史迭代中，有几个反推结论值得记录：

- 当 FPS 很低时，
  不能期望每一帧都做细粒度角度微调，
  搜索策略必须更“粗粒度”，同时依赖较大的容错视场；
- 当 FPS 稍高时，可以适度缩小 search_step_angle，
  提高扫描精度，但仍要考虑云台物理极限；
- 控制频率通常高于推理 FPS，
  但“没有新结果时多做计算”意义不大，
  因此在 `autoaim_action` 中需要慎重安排 sleep 与状态更新逻辑。

这些推断也间接影响了当前 selector、angle 和 pipeline 之间的边界划分。

---

## 4.x 线程模型的边界划分：采集脚本 vs 自瞄技能

在自瞄相关的设计讨论里，曾经出现过一个看似简单、但实际影响边界很大的问题：

> “摄像头采集要不要一开始就做成双线程（采集线程 + 处理线程）？”

这里的“处理”既包括预览窗口（imshow），也包括后续的识别、自瞄逻辑。

当前阶段我们刻意做了一个划分：

- **训练数据采集脚本（`training/data_collector.py`）**  
  - 只负责：从摄像头读帧 + 可选预览 + 手动按键触发保存到磁盘；  
  - 强调：单线程、简单、稳定，方便长时间采集和后续维护；  
  - 实测在树莓派上，640×480 @ 60 FPS（MJPG）条件下，  
    单线程采集 + 预览可以稳定在 ~30–40 FPS，已经满足采集需求；  
  - 结论：**不在采集脚本里引入双线程**，避免过早复杂化。

- **自瞄技能（`src/skill/autoaim.py` + `src/aimassistant/`）**  
  - 目标场景：同时进行“高频采集 + 目标选择 + 角度计算 + 云台控制”，  
    还可能叠加发射控制等逻辑；  
  - 在这种高负载、强实时性场景下，  
    单线程很容易因为推理或控制逻辑变慢而拖死整条采集链；  
  - 因此我们预期在未来版本里，自瞄更适合采用：  
    - **采集线程**：只负责 `camera.read()` 并维护“最新帧”；  
    - **自瞄线程**：基于最新帧做 selector/angle/pipeline + gimbal 控制；  
  - 这样做的收益是：  
    - 推理再慢，摄像头采集也尽量跟得上硬件节奏；  
    - 技能线程总是拿“最新帧”，减少画面滞后感；  
    - 多线程的复杂度集中在自瞄模块内部，不影响训练数据采集脚本。

这一条的设计意图可以简单概括为：

- **采集脚本：单线程优先，稳定和易用最重要；**  
- **自瞄技能：未来根据需要演进到“双线程（采集 + 推理/控制）”模型。**

当你在未来打算给 autoaim 加入更多逻辑（例如在线调参、实时可视化、复杂搜索策略）时，可以回到这里，确认是否是时候把双线程采集真正落地到自瞄技能中了，而不是去修改训练数据采集工具本身。

---

## 5. 典型坑点与未来改造思路

本节集中列出自瞄相关的若干坑点与经验，
同时给出一些未来可能的改造方向（明确：尚未实现）。

### 5.1 selector 相关坑点

- 中心距离的定义偏离直觉：
  - 目前实现使用的是“到 (0,0) 的距离”，
    而非“到画面中心 (0.5, 0.5) 的距离”；
  - 这会导致评分中“居中程度”的含义与人类直觉不完全一致；
  - 如果未来重写 selector，需要优先修正这一点。
- 权重注册机制的可维护性：
  - 通过装饰器在模块加载时叠加权重，
    新增因子时稍不注意就可能超过 1.0；
  - 更好的方案是将权重统一放到配置中，
    或者在 journey 中记录一份当前权重表，便于审查。

### 5.2 角度与 FOV 相关坑点

- FOV 估算：
  - 垂直 FOV 目前是由水平 FOV 和宽高比推算；
  - 如果实际摄像头的水平/垂直视场并不满足这一几何关系，
    自瞄在垂直方向上的精度会受到影响；
  - 未来需要单独的标定脚本和流程，并把结果更新到 `config.py`。
- pitch 范围约束：
  - 机械结构和底层 SDK 对 pitch 角有硬限制；
  - 当前 angle 模块并没有在计算阶段施加这些限制，
    而是依赖 gimbal 模块或底层来兜底；
  - 这一分层设计需要在未来的 `gimbal` journey 中补充，
    以免后续在 angle 中重复实现限制逻辑造成不一致。

### 5.3 状态与接口命名的错配风险

- recognizer 状态字段名：
  - 旧文档和早期代码中多次使用 `inference_fps` 这一名字；
  - 当前 `recognizer` 实现中则可能采用 `actual_inference_fps` 等字段；
  - 如果 auto-aim 依赖了旧字段名，却没有在代码中同步更新，
    search_step_angle 等计算就会落回默认值或错误路径，
    这种 bug 通常不容易一眼发现。
- skill 状态管理边界：
  - 技能线程内部和 `SkillManager` 都能“修改技能状态”，
    如果两边同时负责，容易出现竞态和状态不一致；
  - 当前共识是：技能线程只 log + 退出，
    状态管理尽量集中在 `SkillManager`，
    这一约定应在 future refactor 时保持一致。

### 5.4 未来扩展方向（尚未实现）

以下内容在旧文档与讨论中多次出现，
但截至当前版本并未真正落地实现，
在此仅作为未来工作的参考清单：

- 单目测距模块：
  - 利用装甲板实际尺寸和相机内参估算距离，
    为更精细的射击策略提供输入；
- 轨迹预测与滤波：
  - 使用卡尔曼滤波或其他预测方法，
    在目标高速运动或 FPS 较低时提升命中率；
- 更丰富的 selector 策略：
  - 引入目标类别、威胁度、颜色等额外因子，
    并用配置文件而非硬编码管理权重；
- 不同性能模式：
  - 根据当前 FPS 和场景需求，
    在“快速扫描”“精细跟踪”等模式之间自动切换；
- 与其他 journey 的联动：
  - 在 `vision_recognition_journey.md` 中详细记载 FPS/延迟演进，
    让本篇可以少写性能细节，更多关注策略层历史；
  - 在 `uart_communication_journey.md` 或 gimbal 相关 journey 中，
    记录云台控制接口和反馈机制的演进，
    以避免在本篇重复下层协议的故事。

---

> 小结：
> - 自瞄并不是一个“单文件算法”，而是站在视觉、通信、云台控制
>   和配置管理等多层之上的综合结果；
> - 本文试图把从 v1.1 开始的关键决策与坑点串成一条线，
>   方便之后在做大改动时，有一个可回溯的历史背景；
> - 当你真正开始重写 aimassistant 相关代码时，
>   建议一边查看本篇，一边对照 `src/aimassistant/` 与
>   `src/skill/autoaim.py` 的当前实现，确保“历史经验”与“现状”都在脑中。
